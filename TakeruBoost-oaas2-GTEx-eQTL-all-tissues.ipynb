{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d61452cb-d147-4a1d-a025-0de7206a5443",
   "metadata": {},
   "source": [
    "## Omics Analysis with Apache Spark\n",
    "### GTEx eQTLデータ編 第2回\n",
    "\n",
    " 前回はGTEx eQTLのAdipose_SubcutaneousをApache SparkのDataFrameに取り込みました。今回はいよいよそれを全tissueに拡大します。また、容量も大きくなってきましたので今回はmacではなくAWS上に構築したSpark Clusterを使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c88b3da-0c78-4f1b-b42d-5aeec7538b43",
   "metadata": {},
   "source": [
    "### データの下準備\n",
    "GTExのデータはgzipされていますがApache Sparkで分散処理を行うのにあたりbgzipに変換します。これで分散性能がかなり変わってきます。  \n",
    "gsutilでファイルを取得  ->  gunzip  ->  bgzip  ->  s3へ配置  \n",
    "という流れで前準備を行っています。\n",
    "\n",
    "\n",
    "### 起動\n",
    "あらかじめ設定しておいたAWSの\"Service Catalog\"からパラメータを入力し、欲しいサイズのSpark Clusterを起動していきます。  \n",
    "今回の処理に必要最低限なオプションは以下のとおりです。  \n",
    "\n",
    "|オプション|役割等|\n",
    "|---|---|\n",
    "|--packages| packages以下にコンマ区切りで依存関係にあるパッケージを指定しています。|\n",
    "|io.projectglow:glow-spark3_2.12:1.1.2, <br> io.delta:delta-core_2.12:1.0.1, |この２行はdeltaを使うために必要です。|\n",
    "|org.apache.hadoop:hadoop-aws:3.2.0, |s3へアクセスするために必要なものです。|\n",
    "|--conf spark.driver.memory=12G| driverに12GBのメモリを割り当てています。<br>今回はClusterではないのですがその場合はdriverがexecutorの役割を担うためこのようにしています。|\n",
    "|--conf spark.serializer=org.apache.spark.serializer.KryoSerializer|serializeにつかうclassを指定。推奨値です。|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b1455-4df6-44e0-b4b5-2d59be5dbb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = ' \\\n",
    " --conf spark.jars.packages=\\\n",
    "io.projectglow:glow-spark3_2.12:1.1.2,\\\n",
    "io.delta:delta-core_2.12:1.0.1,\\\n",
    "org.apache.hadoop:hadoop-aws:3.2.0, \\\n",
    " --conf spark.hadoop.io.compression.codecs=io.projectglow.sql.util.BGZFCodec \\\n",
    " --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \\\n",
    " --master yarn pyspark-shell '\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()#\n",
    "\n",
    "spark\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec6f86c-46c7-431d-a1f6-193e9385f84b",
   "metadata": {},
   "source": [
    "|命令文|役割等|\n",
    "|---|---|\n",
    "|from pyspark import SparkContext, SparkConf, SQLContext<br>from pyspark.sql import SparkSession|SparkSessionとSparkContextをつくるのに必要です。|\n",
    "|sparkConf = SparkConf()<br>sparkConf.setAppName('GTEx eQTL data test')| Sparkの設定を記述できます。<br>今回は必要な設定は先のPYSPARK_SUBMIT_ARGSに入れています。|\n",
    "|spark = SparkSession.builder.config(conf = sparkConf).getOrCreate() |Spark Sessionを作成しています。|\n",
    "|sc = spark._sc|作成したSparkSessionより、SparkContextを得ています。|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263b666d-904b-451e-b779-4ab57efe788e",
   "metadata": {},
   "source": [
    "### データの変換\n",
    "前回と同様に、variant_idは chr1_13550_G_A_b38のように複数の情報がひとつになっていてこのままでは利活用に支障があると思われるため、次のような列に変換をすることにします。\n",
    "また、後で他のtissueも結合しますので新たに「tissue」を追加します。  \n",
    "スキーマも前回と同じものを使えます。\n",
    "\n",
    "|列名|内容|\n",
    "|---|---|\n",
    "|contigName|chr1|\n",
    "|start|13550|\n",
    "|referennceAllele|G|\n",
    "|alternateAlleles|A|\n",
    "|tissue|Adipose_Subcutaneous|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867e0a39-fa43-4d61-8bda-c5923d871be1",
   "metadata": {},
   "source": [
    "以降で使うモジュールをロードし、ファイル名とtissueを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17da62b-4f3c-4b8d-8421-255d76f575a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit,split,col,regexp_replace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ab0f51-b67b-4978-a1df-ef8a2ca00ac9",
   "metadata": {},
   "source": [
    "ファイルのリストから、inputファイルのリストやoutputファイルのリストを作成しておきます。  \n",
    "今回はs3とhdfsの両方に出力していますが本来はどちらか一方でよいと思います。  \n",
    "処理の内容で中間ファイルとして集めるものが多ければhdfsの方が高速ですし、  \n",
    "そういった要素が少なく、最後に結果を保存するのみであればs3だけでよさそうです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e56ddd3-d6b7-4688-b8d6-daac58fd21be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "list_eqtl_files_gz = subprocess.run([\"aws s3 ls \\\n",
    "  --recursive s3://[Your buckets containing omics-data-raw] /GTEx_Analysis_v8_QTLs/GTEx_Analysis_v8_eQTL_all_associations/ \\\n",
    "  |grep bgz$|awk '{print \\\"s3://ut-omics-data-raw/\\\"$4}'\"],\n",
    "                                stdout=subprocess.PIPE,shell=True)\n",
    "\n",
    "eqtl_files_gz = list_eqtl_files_gz.stdout.decode('utf-8').split()\n",
    "# input fileのリスト\n",
    "with open(\"gtex_eQTL_paths_in.txt\", \"w\") as f:\n",
    "    for eqtl_file in eqtl_files_gz:\n",
    "        f.write(f\"{eqtl_file}\\n\")\n",
    "\n",
    "# hdfsへのoutput fileのリスト\n",
    "with open(\"gtex_eQTL_paths_hdfs_out.txt\", \"w\") as f:\n",
    "    for eqtl_file in eqtl_files_gz:\n",
    "        eqtl_file_out = eqtl_file.replace(\"s3://[Your buckets containing omics-data-raw] \", \"/user/beowulf/gtex-delta\").replace(\".bgz\", \".delta\")\n",
    "        f.write(f\"{eqtl_file_out}\\n\")\n",
    "\n",
    "\n",
    "# すべてのeQTLデータをまとめたDataFrameの保存先\n",
    "gtex_eqtl_all_delta_s3_path = \\\n",
    "  's3a://[User buckets for omics-data-output ]/gtex-delta/GTEx_Analysis_v8_QTLs/GTEx_Analysis_v8_eQTL_all_associations/all_delta'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd9b7ed-e4e7-4b80-893d-3cf41e5634b2",
   "metadata": {},
   "source": [
    "リストをもとにbgzファイルを読み込み、前回つくったSchemaを使ってDataFrameに変換します。    \n",
    "同時に、sparkのunionでつなげていき、1つの大きなDataFrameにしています。  \n",
    "個々のtissueはそれぞれdelta形式でs3へ保存しています。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac2d4f-29ed-4440-832a-dd058ad19bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit,split,col,regexp_replace\n",
    "\n",
    "with open(\"gtex_eQTL_paths_in.txt\") as f:\n",
    "    eqtl_files = f.read().splitlines()\n",
    "\n",
    "for gtex_eqtl_tmp_path in eqtl_files:\n",
    "    name = \"GTEx_eQTL_allpairs_\" + gtex_eqtl_tmp_path.split(\".\")[0].split(\"/\")[-1]\n",
    "    print(name)\n",
    "    tissue =  gtex_eqtl_tmp_path.split(\".\")[0].split(\"/\")[-1]\n",
    "    gtex_eqtl_tmp_delta_hdfs_path = f\"gtex-delta/GTEx_Analysis_v8_QTLs/GTEx_Analysis_v8_eQTL_all_associations/{tissue}_delta\"\n",
    "    schema = StructType([\n",
    "      StructField(\"gene_id\", StringType(), False),\n",
    "      StructField(\"variant_id\", StringType(), False),\n",
    "      StructField(\"tss_distance\", IntegerType(), False),\n",
    "      StructField(\"ma_samples\", IntegerType(), False),\n",
    "      StructField(\"ma_count\", IntegerType(), False),\n",
    "      StructField(\"maf\", DoubleType(), False),\n",
    "      StructField(\"pval_nominal\", DoubleType(), False),\n",
    "      StructField(\"slope\", DoubleType(), False),\n",
    "      StructField(\"slope_se\", DoubleType(), False),\n",
    "    ])\n",
    "    gtex_eqtl_tmp = spark.read.option(\"inferSchema\",False) \\\n",
    "                    .option(\"delimiter\",\"\\t\") \\\n",
    "                    .csv(gtex_eqtl_tmp_path, header=True,schema=schema)\n",
    "    gtex_eqtl_tmp = gtex_eqtl_tmp.\\\n",
    "        withColumn(\"tissue\",lit(tissue)).\\\n",
    "        withColumn(\"contigName\",split(col(\"variant_id\"), \"_\").getItem(0)).\\\n",
    "        withColumn(\"start\",split(col(\"variant_id\"), \"_\").getItem(1).cast(IntegerType())).\\\n",
    "        withColumn(\"referenceAllele\",split(col(\"variant_id\"), \"_\").getItem(2)).\\\n",
    "        withColumn(\"alternateAlleles\",split(col(\"variant_id\"), \"_\").getItem(3)).\\\n",
    "        select(\"tissue\",regexp_replace(col(\"contigName\"),\"chr\",\"\").\\\n",
    "               alias(\"contigName\"),\"start\",\"referenceAllele\",\"alternateAlleles\",\"gene_id\",\"variant_id\",\"tss_distance\",\\\n",
    "              \"ma_samples\",\"ma_count\",\"maf\",\"pval_nominal\",\"slope\",\"slope_se\")\n",
    "    if 'df' in locals():\n",
    "        df = df.union(gtex_eqtl_tmp)\n",
    "    else:\n",
    "        df = gtex_eqtl_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8c1aca-9a00-4b42-aa92-cdc909a716ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ディスクへの書き込み、読み込み\n",
    "##### S3\n",
    "結合したeQTLデータをdelta形式でs3へ保存します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50d93ef-5058-4210-a166-4cefbe79a70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(gtex_eqtl_all_delta_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b082c1-f2bc-459e-8413-948746c799b8",
   "metadata": {},
   "source": [
    "s3から読み込む場合は、このようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb60c290-bbbc-497a-b603-fe0ae2583de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s3 = spark.read.format(\"delta\").load(gtex_eqtl_all_delta_s3_path).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb1ac3a-1285-42d7-98f9-4b5c3898d71c",
   "metadata": {},
   "source": [
    "読み込んだDataFrameを確認します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f29ea3-7456-42b7-b5c0-900920566cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d39543-15dc-4886-87ca-dc7e881b622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02faf7d1-7215-4795-bdf8-1002c5e076cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s3.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1cb18e-8426-427a-ad5b-5042f411a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950ec313-8760-4ade-aeee-57a4bff53749",
   "metadata": {},
   "source": [
    "### DataFrame操作\n",
    "読み込んだDataFrameは次のようにFilterなどの操作ができます。  \n",
    "ここでは8番染色体のみに絞り込み、pval_nominalをsortしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6279133-5aff-45a3-bd79-8b0a7bd47e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_s3.filter(df_s3.contigName == 8).orderBy(\"pval_nominal\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b5042-d345-4754-aa27-1ae9436aa1fa",
   "metadata": {},
   "source": [
    "filterを追加してmaf < 0.1のみを表示させてみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee8b471-51ad-4f60-914b-74c7fa2ac823",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_s3.filter(df_s3.contigName == 8).orderBy(\"pval_nominal\").filter(df_s3.maf < 0.1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a4d88b-b57e-4379-b3d3-d9dac82edacf",
   "metadata": {},
   "source": [
    "参考で、2回目以降はメモリに載ってくるので高速になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef7151-6c53-42da-9804-91a03b4848bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_s3.filter(df_s3.contigName == 8).orderBy(\"pval_nominal\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1575163-eb9a-42d2-8649-2e467bbba627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
